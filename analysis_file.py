# -*- coding: utf-8 -*-
"""Analysis File.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rp8lqpvGxDNXZySBx4OCJtJd2gqJwdXs

# Importing the libraries
"""

import pandas as pd
import nltk
import string
import re

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download("cmudict")

"""# Additional Functions"""

from nltk.corpus import cmudict
pronouncing_dict = cmudict.dict()
# Counting the number of syllables
def count_syllables(word):
    if word.lower() in pronouncing_dict:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in pronouncing_dict[word.lower()]])
    else:

        vowels = "AEIOUaeiou"
        syllable_count = 0
        in_syllable = False
        for char in word:
            if char in vowels:
                if not in_syllable:
                    syllable_count += 1
                    in_syllable = True
            else:
                in_syllable = False
        return syllable_count


# Counting the number of syllables with the condition
def count_syllable_new(word):

    if word.endswith("es") or word.endswith("ed"):
        word = word[:-2]

    syllable_count = 0

    prev_was_vowel = False

    vowels = "aeiouAEIOU"

    for char in word:

        if char in vowels and not prev_was_vowel:
            syllable_count += 1
        prev_was_vowel = char in vowels


    if syllable_count == 0:
        syllable_count = 1

    return syllable_count

# Calculating the number of pronouns

def findPronouns(text):
    pattern = r'\b(?i)(?<!\bUS\b)(I|we|my|ours|us)\b'
    matches = re.findall(pattern, text)
    pronoun_count = len(matches)

    return pronoun_count

url_id_list = []
url_link_list = []
pos_score_list = []
neg_score_list = []
pol_score_list = []
sub_score_list = []
sentence_len_list = []
perc_complex_list = []
fog_index_list = []
word_count_sentence_list = []
syllable_count_list = []
complex_word_count_list = []
word_count_list = []
personal_pronoun_list = []
avg_word_length_list = []

"""# Text Analysis


1.   **tokenize_sentence:** The list of the text being tokenized to the sentences
2.   **number_of_sentences:** Length of the tokenize_sentence list
3.   **tokenize_word_1:** This is original list which has all the words from the text
4. **tokenize_word:** A list of words, where every word is in lowercase
5. **tokenize_word_new:** A list of words from tokenize_word where every punctuation mark has been removed.
6. **tokenize_word_1**: A list of words from tokenize_word_1 where punctuation marks have been removed.
7. stop_word_lower = A list of all the defined stop words which are in lower case
8. filter_word: A list of filtered words after removing the stop words


"""

def analysis(text):


  #Sentence Tokenization

  from nltk.tokenize import sent_tokenize
  tokenize_sentence = sent_tokenize(text)
  number_of_sentences = len(tokenize_sentence)


  #Work Tokenization

  from nltk.tokenize import word_tokenize
  tokenize_word_1 = word_tokenize(text)

  # Converting it into a list with all lowercase words
  tokenize_word = [word.lower() for word in tokenize_word_1]
  #Remove all the punctuation marks from the list
  tokenize_word_new = [''.join(char for char in word if char not in string.punctuation) for word in tokenize_word]
  tokenize_word_new = [word for word in tokenize_word_new if word]


  #Removing the punctuation marks from a list where all characters arent lowercase
  tokenize_word_1_new = [''.join(char for char in word if char not in string.punctuation) for word in tokenize_word_1]
  tokenize_word_1_new = [word for word in tokenize_word_1_new if word]

  word_len = len(tokenize_word_new)


  #Removing stop words:
  file_names = ['StopWords_Auditor.txt', 'StopWords_Currencies.txt', 'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt', 'StopWords_GenericLong.txt', 'StopWords_Geographic.txt', 'StopWords_Names.txt']
  stop_words_def = []

  for file_name in file_names:
    with open(file_name, 'r', encoding="cp1252") as f:
        stop_words_def.extend(f.read().splitlines())

  stop_words_def_lower = [word.lower() for word in stop_words_def]
  filter_word = []
  for word in tokenize_word_new:
    if word not in stop_words_def:
        filter_word.append(word)


  #Lemmatization:
  from nltk.stem.wordnet import WordNetLemmatizer
  lem = WordNetLemmatizer()

  lemmatize_word = []
  for w in filter_word:
    corp = w.lower()
    lemmatize_word.append(lem.lemmatize(corp))


  #Defining the negative dictionary
  with open('negative-words.txt', 'r', encoding='cp1252') as f:
    neg_words_1 = f.read().split()

  neg_words = [word.lower() for word in neg_words_1]


  #Defining the positive dictionary
  with open('positive-words.txt', 'r', encoding="cp1252") as f:
    pos_words_1 = f.read().split()

  pos_words = [word.lower() for word in pos_words_1]



  #Performing the analysis

  pos_score = 0
  neg_score = 0
  total_score = len(lemmatize_word)
  for w in lemmatize_word:
    if w in pos_words:
      pos_score+=1
    elif w in neg_words:
      neg_score-=1
    else:
      pass
  neg_score = -1 * neg_score

  #Assigning positive and negative score
  pos_score_list.append(pos_score)
  neg_score_list.append(neg_score)

  #Evaluating the polarity
  pol_score = (pos_score - neg_score)/((pos_score + neg_score) + 0.000001)
  pol_score_list.append(pol_score)


  #Evaluating the subjectivity score
  sub_score = (pos_score + neg_score)/((total_score)+0.000001)
  sub_score_list.append(sub_score)


  #Average sentence length
  avg_sentence_len = word_len/number_of_sentences
  sentence_len_list.append(avg_sentence_len)

  #Percentage of Complex Words
  complex_words_1 = [word for word in lemmatize_word if count_syllables(word) > 2]
  complex_words_len = len(complex_words_1)
  percentage_of_complex_words = complex_words_len/word_len
  perc_complex_list.append(percentage_of_complex_words)

  #Fog Index
  fog_index = 0.4 * (avg_sentence_len + percentage_of_complex_words)
  fog_index_list.append(fog_index)

  #Average number of words per sentence
  word_count_sentence_list.append(word_len/number_of_sentences)

  #Complex Word Count
  complex_word_count_list.append(complex_words_len)

  #Word Count
  word_count_list.append(word_len)

  #Syllable per word
  sum_syllable_count = 0
  avg_syllable_count = 0

  for w in lemmatize_word:
    sum_syllable_count += count_syllable_new(w)

  avg_syllable_count = sum_syllable_count/total_score
  syllable_count_list.append(avg_syllable_count)

  #Personal Pronouns
  count_pronouns = 0

  for w in tokenize_word_1_new:
    count = findPronouns(w)
    count_pronouns+=count
  personal_pronoun_list.append(count_pronouns)

  #Average Word Length
  total_number_characters = 0
  for w in lemmatize_word:
    total_number_characters+=len(w)
  avg_word_length = total_number_characters/word_len
  avg_word_length_list.append(avg_word_length)

"""# Iterating through each and every text file"""

df = pd.read_csv('/content/drive/MyDrive/BlackCoffers Internship/Internship Assignment/Data/Output Data Structure - Sheet1.csv')
url_id_list = []
url_link_list = []
col_name = "URL_ID"
text_files =[]
for index, row in df.iterrows():
    value = row[col_name]
    text_files.append(value)

for url_id in text_files:
    path = r'/content/drive/MyDrive/BlackCoffers Internship/Internship Assignment/Data/Articles/{}.txt'.format(url_id)
    with open(path, "r") as f:
        text1 = f.read()

    # Perform text analysis and append values to respective lists
    analysis(text1)

    # Append URL_ID and URL to their respective lists
    url_id_list.append(url_id)
    url_link_list.append(df[df['URL_ID'] == url_id]['URL'].values[0])

data_dict = {
    'URL_ID': url_id_list,
    'URL': url_link_list,
    'POSITIVE SCORE': pos_score_list,
    'NEGATIVE SCORE': neg_score_list,
    'POLARITY SCORE': pol_score_list,
    'SUBJECTIVITY SCORE': sub_score_list,
    'AVG SENTENCE LENGTH': sentence_len_list,
    'PERCENTAGE OF COMPLEX WORDS': perc_complex_list,
    'FOG INDEX': fog_index_list,
    'AVG NUMBER OF WORDS PER SENTENCE': word_count_sentence_list,
    'COMPLEX WORD COUNT': complex_word_count_list,
    'WORD COUNT': word_count_list,
    'SYLLABLE PER WORD': syllable_count_list,
    'PERSONAL PRONOUNS': personal_pronoun_list,
    'AVG WORD LENGTH': avg_word_length_list
}

df1 = pd.DataFrame(data_dict)

df1 = df1.reset_index(drop=True)

df1

df1.to_csv('/content/drive/MyDrive/BlackCoffers Internship/Internship Assignment/Data/Output Data Structure - Sheet1.csv', index=False)

